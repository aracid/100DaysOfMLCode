# 100DaysOfMLCode
Repository of the AI journey 100 Days of ML Code
Total Time : 2 Hours

# 100 Days Of ML Code - Log

## Day 1 : October 9th 2018
1 Hour

### Introduction into Q Learning
Difference between On Policy and Off Policies.

- Q Learning is Off Policy Method.
- Temporal differences is similar to a moving average.
- Adaptive learning rate - to assist in speed. If Alpha is too high, we may miss the sweet spot, if its too low, then its to slow to train.

### Neural Material Synthesis
Neural Material Synthesis is something that interests me, and as I am currently working on an animated feature that requires synthetic textures, this approach would make sense.

### Alpha go has been revised
  1) Only self play
  2) Predefined features, no hand crafted features.
  3) Change from inception to res-net (Residual)
  4) Combined policy and value network

### Variational Autoencoders
Compress data into a space
AutoEncoder - represent high dimensional data into low dimensional data.
VAE is different because it has 2 terms, mean and standard deviation.


**Links:**
[Lecture 4.5 Q-Learning Tutorial of Move 37](https://www.youtube.com/watch?v=tU6_Fc6bKyQ)
[Neural Material Synthesis, This Time On Steroids](https://www.youtube.com/watch?v=UkWnExEFADI&feature=em-uploademail)
[How AlphaGo Zero works - Google DeepMind](https://www.youtube.com/watch?v=MgowR4pq3e8)
[Variational Autoencoders](https://www.youtube.com/watch?v=9zKuYvjFFS8)


## Day 2 : October 10th 2018
1 Hour

### Proximal Policy Optimization

[Policy Gradient methods and Proximal Policy Optimization (PPO): diving into Deep RL!](https://www.youtube.com/watch?v=5P7I-xPq8u8)
- Learns online.
- Learns directly from the environment.
I didn't quiet follow and will have to re-watch

### Reinforcement Learning
[An introduction to Reinforcement Learning](https://www.youtube.com/watch?v=JgvyzIkgxF0)
The network that transforms input frames to output actions is called the policy network.

### Overcoming sparse rewards in Deep RL: Curiosity, hindsight & auxiliary tasks.
[Overcoming sparse rewards in Deep RL: Curiosity, hindsight & auxiliary tasks.](https://www.youtube.com/watch?v=0Ey02HT_1Ho)


## Day 3 : October 11th 2018
45 Min
[A Short Introduction to Entropy, Cross-Entropy and KL-Divergence](https://www.youtube.com/watch?v=ErfnhcEV1O8)
The best explanation of Log I've come across.
Entropy : The average amount of information you get from one sample drawn given from the probability distribution P.

[TensorFlow 2.0 Changes](https://www.youtube.com/watch?v=WTNH0tcscqo)

## Day 3 : Octover 12th 2018
4 Hours
[IBM COMMUNITY DAY: AI - Building Custom AI Web Services](https://www.ibmai-platform.bemyapp.com/#/conference/5bb5415cb4ae3f00044cb9fc)

*Links*
Daily Reading Material
https://arxiv.org/list/cs.AI/recent
https://arxiv.org/list/cs.LG/recent

# Concepts to understand

Trust Region Policy Optimization - John Schulman Berkeley
Q-Learning
Autoencoders
Temporal Difference
Reinforcement learning
Monte Carlo Tree Search
neural impainting
Reinforcement Learning
Disentangled VAE

Outcome of course -
Denoise
Voice recognition - speech to text.
Text to image classification, ie find an image based on text, the text isnt a category but a convnet.
Form a sentence and form various images based on the sentence.

*Need to read*
- Disentangled VAE's (DeepMind 2016): https://arxiv.org/abs/1606.05579
- Applying disentangled VAE's to RL: DARLA (DeepMind 2017): https://arxiv.org/abs/1707.08475
- Original VAE paper (2013): https://arxiv.org/abs/1312.6114
- Reinforcement Learning with Unsupervised Auxiliary Tasks - DeepMind:https://arxiv.org/abs/1611.05397
- Curiosity Driven Exploration - UC Berkeley: https://arxiv.org/abs/1705.05363
- Hindsight Experience Replay - OpenAI: https://arxiv.org/abs/1707.01495



*Need to watch*
[Deep RL Bootcamp](https://sites.google.com/view/deep-rl-bootcamp/lectures?authuser=0)


# Authors to follow

# Vloggers to follow
[Arxiv Insights](https://www.youtube.com/channel/UCNIkB2IeJ-6AmZv7bQ1oBYg)
